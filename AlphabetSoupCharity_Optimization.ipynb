{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# AlphabetSoupCharity Optimization\n\nThis notebook focuses on optimizing the deep learning model for Alphabet Soup Charity's funding success predictions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Import Dependencies"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense\n", "from tensorflow.keras.callbacks import ModelCheckpoint"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Load and Preprocess Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load the dataset\n", "df = pd.read_csv(\"charity_data.csv\")\n\n", "# Drop unnecessary columns\n", "df = df.drop(columns=[\"EIN\", \"NAME\"])\n\n", "# Bin rare categories\n", "app_type_counts = df[\"APPLICATION_TYPE\"].value_counts()\n", "app_types_to_replace = app_type_counts[app_type_counts < 500].index\n", "df[\"APPLICATION_TYPE\"] = df[\"APPLICATION_TYPE\"].replace(app_types_to_replace, \"Other\")\n\n", "# One-hot encode categorical variables\n", "df = pd.get_dummies(df)\n\n", "# Define features and target\n", "X = df.drop(columns=[\"IS_SUCCESSFUL\"])\n", "y = df[\"IS_SUCCESSFUL\"]\n\n", "# Split the data\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "# Scale the data\n", "scaler = StandardScaler()\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Build the Optimized Neural Network Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define the optimized model\n", "optimized_model = Sequential([\n", "    Dense(100, activation='relu', input_dim=X_train_scaled.shape[1]),\n", "    Dense(50, activation='relu'),\n", "    Dense(20, activation='relu'),\n", "    Dense(1, activation='sigmoid')  # Binary classification\n", "])\n\n", "# Compile the model\n", "optimized_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n", "# Add a callback for saving the model\n", "checkpoint = ModelCheckpoint(\"AlphabetSoupCharity_Optimization.h5\", save_weights_only=False, save_freq='epoch')\n\n", "# Train the model\n", "history = optimized_model.fit(X_train_scaled, y_train, epochs=100, batch_size=16, callbacks=[checkpoint])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Evaluate the Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss, accuracy = optimized_model.evaluate(X_test_scaled, y_test)\n", "print(f\"Optimized Loss: {loss}, Optimized Accuracy: {accuracy}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Save the Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimized_model.save(\"AlphabetSoupCharity_Optimization.h5\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}